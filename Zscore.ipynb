{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The usual imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from ipywidgets import interactive, interact, fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A plotting method that will be very helpful down the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _plot_over_time_3d(sequence: np.array, fig, ax, n: int):\n",
    "    assert isinstance(n, int)\n",
    "    ax.clear()\n",
    "    ax.scatter(sequence[:n, 0], sequence[:n, 1], sequence[:n, 2])\n",
    "    display(fig.canvas)\n",
    "def plot_over_time_3d(sequence: np.array):\n",
    "    plt.ioff();fig = plt.figure(); plt.ion()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    return interact(_plot_over_time_3d, sequence=fixed(sequence), fig=fixed(fig), ax=fixed(ax), n=(0, len(sequence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Observation\n",
    "This began with a though about Z-scoring data. Given a set of datapoints $\\mathbf{x} = [x_1, x_2, ..., x_n]$, applying a Z-score ($z_i = \\frac{x_i - \\mu}{\\sigma}$ where $\\mu = \\frac{1}{n}\\sum_{i=1}^{n} x_i$) can be viewed as a function $Z: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n$. In that light, what happens if we apply $Z\\circ Z\\circ ... \\circ Z (\\mathbf{x}) = Z^{(k)}(\\mathbf{x})$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e26ccc0db4624bffbd17e40d90e7945e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=5, description='n', max=11), Output()), _dom_classes=('widget-interact',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n, k = 3, 10\n",
    "seed = np.random.randn(n)\n",
    "sequence = [seed]\n",
    "for _ in range(k):\n",
    "    sequence.append(zscore(sequence[-1]))\n",
    "sequence = np.stack(sequence)\n",
    "plot_over_time_3d(sequence);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this is not particularly suprising (that we only get two points), since if $\\mathbf{z} = Z(\\mathbf{x})$, then $\\mu_{\\mathbf{z}} = 0$ and $\\sigma_{\\mathbf{z}} = 1 \\rightarrow Z(\\mathbf{z}) = \\mathbf{z}$.\n",
    "\n",
    "However, what happens if instead we say $Z'(\\mathbf{x}) = |\\frac{\\mathbf{x}-\\mu}{\\sigma}|$? This has a real-world interpretation: the **magnitude** of the deviation from the mean (when we don't care about the direction of deviation, just how large it is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d6b44d369d43cba2083f356362dfb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=250, description='n', max=501), Output()), _dom_classes=('widget-interac…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n, k = 3, 500\n",
    "seed = np.random.randn(n)\n",
    "sequence = [seed]\n",
    "Z = lambda x: np.abs(zscore(x))\n",
    "for _ in range(k):\n",
    "    sequence.append(Z(sequence[-1]))\n",
    "sequence = np.stack(sequence)\n",
    "plot_over_time_3d(sequence);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see slowly but surely, a definite pattern begins to emerge! In fact it appears as though all the points (with the exception of the seed point) in our sequence are constrained to some kind of spherical object. Why is this? Let's observe the relationship between the mean and the variance of each of the points in our sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b69d5fbbcc3c4615a2795a1ab3d9c95a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = list(range(sequence.shape[0]))\n",
    "means = np.mean(sequence, axis=1)\n",
    "variances = np.var(sequence, axis=1)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(index, means, label='Mean')\n",
    "ax.scatter(index, variances, label='Variance')\n",
    "ax.scatter(index, means**2 + variances, label='Mean^2 + Variance')\n",
    "ax.set_xlabel('Sequence Index');ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the $\\mu^{2} + \\sigma^{2} =1$ for all points in the sequence outside of the seed. And this actually makes a lot of sense because if we consider $Z(\\mathbf{x})$ as the values for a categorical random variable $\\Xi$ with uniform PMF, then we get that $\\Xi$ has mean 0 and variance 1 ($\\rightarrow \\mathbf{E}(\\Xi^2)=1$). When we take the absolute value, we will get that the mean and variance change, however $\\mathbf{E}(\\Xi^2)$ will remain the same! So by the definition of variance: $Var(\\Xi) = \\mathbf{E}(\\Xi^2)-\\mathbf{E}(\\Xi)^2 \\rightarrow 1 = Var(\\Xi) + \\mathbf{E}(\\Xi)^2$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In n-dimesnions, it is not too hard to see that in a Euclidean sense, this is sample from a shifted/scaled version of the n-dimensions hypersphere. I think this is more interesting, however, because of the potential to be sampling from a constant-error hypothesis space (using the fact that decomposition of Generalized Error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
